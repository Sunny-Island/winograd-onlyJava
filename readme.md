## 组员
* 赵家贝 清华大学软件学院研二 zjb20@mails.tsinghua.edu.cn
* 谭新宇 清华大学软件学院研二 tanxinyu@apache.org
* 宋一唱 清华大学软件学院研二

## 配置环境

	a. 需要包含测试平台的硬件、软件环境信息，保证可复现性；

## 第一题：Winograd
###　特性分析
- 缓存命中缓存一致性
### 优化思路
- 并行
卷积运算的过程是可以进行并行计算的，比如示例代码中给出的计算中，每一个 channel 的计算就是完全互不干扰，可以并行的。要想加速卷积运算，需要充分利用计算平台多核的优势，尽可能在每一个环节把计算并行化。我们在算法中通过 openmp 库，进行了并行计算的尝试。

- SIMD/向量化
单指令流多数据流（英语：Single Instruction Multiple Data，缩写：SIMD）是基于硬件的支持，微处理器中一个控制器可以控制多个平行的处理微元，对一组数据（数据向量）进行计算，是一种常用于加速计算密集的并行任务的优化方法。
1997 年，x86 扩展出了 MMX 指令集，伴随着 80-bit 的 vector 寄存器，首开向量化计算的先河。 之后，x86 又扩展出了 SSE 和 AVX 指令集。SSE 和 AVX 各有 16 个寄存器。SSE 的 16 个寄存器为 XMM0-XMM15，AVX 的 16 个寄存器为 YMM0-YMM15。同一个寄存器可以装载多个 float 数，并且可以用相应的指令同时进行运算。在我们的服务器和赛方提供的服务器上，我们依次实验了 AVX128，AVX256 和 AVX512 指令集。
算法优化
本题中采用的 Winograd 算法是一种减少乘法次数，增加加法次数，从而对矩阵乘法加速的算法。在示例代码中的 winograd 算法实际上是一种一维算法，需要逐个对每个图片每个 channel 的每一行每一列与每个卷积核相乘，因此有大量的 for 循环。通过查阅文献，我们发现在 3D 卷积运算中，相对于示例代码我们可以做出三个优化：
  - 转换 1D 的 winograd 算法到 2D。比如示例的 winograd(2,3) 可以转换成 winograd(2x2,, 3x3), 可以同时计算得到一个 (2x2) 的输出。
  - 转换 1D 的 winograd 算法到 3D。相关文献指出，在 3D 的矩阵运算中，可以把 winograd 算法分成 4 步，分别是 Input transform、Filter transform、Matrix Multiplication、Output transform。
![](https://s2.ax1x.com/2019/04/29/E1qSoR.png)
  - 扩展 winograd(2,3) 算法到 winograd(4,3)。根据相关文献指出，4x3 算法能更多地减少乘法运算的比例，从而带来更好的加速效果。而且 4x3 将矩阵分块更大一些，对增加缓存命中也相对友好。

根据文献的实验结果，3D-winograd(2,3) 算法是最节能的，适合在 FPGA 等边缘设备上部署。但 3D-winograd(4,3) 算法前向传播是最快的，因此我们最终采取了 3D-winograd(4,3) 算法。

- 缓存/cache miss
在计算密集的并行计算中，要尽可能地利用好缓存，精心设计程序以减少缓存到内存的数据读取。同时多线程的环境下还会出现伪共享问题，大大降低程序的性能。

# 实现/工程细节
- 缓存

在优化时，我们按照程序中的提示，先从优化矩阵计算出发。示例代码的矩阵运算显然是一种对缓存的低效利用，每次读取一段内存，但只用到了其中的第一个数据，下次循环又要去读取新的内存。通过交换循环的顺序，即可提升对缓存的利用效率。我们对矩阵乘法运算做了如下的改进，程序性能就有了 10 倍的提升 (0.22GFlops->3.05GFlops)。

- simd avx128

把矩阵运算划分成一部分 4x4 矩阵运算，不能被 4 整除的部分单独做 1x1 的矩阵运算。并通过 SIMD-avx128 指令集计算 4x4 矩阵运算，在 avx128 中一个寄存器可以存储 4 个 float 数，也可以同时对 4 的 float 做向量化计算。在 4x4 的运算中没有循环，全部做循环展开做计算。同时加入 fma 指令集引入积和熔加运算，进一步压缩指令。此处优化有近 6 倍的提升 (3.05GFlops->12.88 GFlops)

- 多线程 无效到有效 分配
  
此时，我们尝试通过引入 openmp 多线程进行矩阵运算，但多线程一直有负优化的效果，甚至经过实验发现有线程数加倍性能减半的现象。经过分析发现在示例代码中有大量的小矩阵运算，比如在第一部分逐 channel 逐卷积核的矩阵相乘中有大量的大小为 4 的矩阵，矩阵太小导致分给每个线程的任务过少，线程本身的开销大于多线程带来的收益，因此多线程在这种运算中是一种负优化。

为了引入并行加快速度，我们单独为中间的大矩阵运算编写了加入了多线程的函数，小矩阵运算则不加多线程。这一调整有近 5 倍的提升。(12.88GFlops->57.11GFlops)

- perf 调优，寻找瓶颈
此时，我们通过 perf 查看程序各部分的耗时分布。发现如下的问题：
	- 矩阵初始化耗时过长，应该改为 memset，这一调整有 3%的提升 (57.11GFlops->60.86GFlops)
	- winograd 函数中大矩阵计算前后的两个部分耗时过长，这一部分属于算法设计问题，应该对整个算法进行调整。
  
- 加入 avx256
此时我们更换了一台支持 avx2 指令集的服务器同时指令集换成 avx256，在 avx256 中每个寄存器可以存储 8 个 float 数，因此将原来 4x4 矩阵运算更换成 8x8 矩阵运算，这一调整有 1.5 倍的提升 (60.86GFlops->97GFlops)。

编译器优化
llvm gcc 尝试 (97-106GFlops)

升级到 avx512 win4x3 优化 && 3d 矩阵乘法优化

最终在赛方提供的服务器上，我们重新写了 winograd 算法，更新成 3D-winograd(4x3) 算法。在这里，我们分四个部分详细介绍该算法的实现
- Input transform

首先把输入矩阵拆分成 tiles，在这里 tiles 是长宽均为 6 的矩阵，在拆分过程中为了充分发挥向量化的作用，每一次会同时对 16 个 6x6 的 tile 矩阵进行操作，因为每个 avx512 寄存器能存储 16 个 float 数。通过循环展开手写 avx 指令的方式，把 16 个矩阵中的数读取出来并重新排列，之后再批量与矩阵 G 相乘，相乘操作也全部展开用 avx 指令的方式实现。这样批量操作可以充分发挥单核 SIMD 指令的效能，并且每个计算单元不会影响其他部分的内存，不会出现伪共享问题，可以实现完全的并行操作。大量的循环展开虽然增加了代码量，但有利于减少循环开销、也有利于指令流水线的调度。对于边长不满足被步长整除（步长为 4/64，即无法被 4/64 整除）的参数，可以进行 padding。
- Filter transform
因为卷积核是固定的 3x3 大小，SIMD 无法发挥作用，因此这里使用了循环展开和多线程进行计算卷积核与矩阵 G 相乘，因为卷积核之间也是互相独立的，因此可以尽可能的增加并行度加快运算。最后的赋值也使用#pragma unroll 指示编译器进行循环展开。

- Matrix Multiplication

经过测试发现，使用 mkl 替换我们自己的矩阵有较大的提升（约 3 倍提升），因此这里全部替换成 mkl 的 sgemm 函数。这里的并行度有两个参数，一个是 36，因为经过 Input transform 得到的三维矩阵有一维是 36，且彼此之间互不影响，可以独立并行。另一个参数是 batch，因为之前把 batch 中所有图像拼在了一起，这里也可以并行，相当于对每个图像做计算。

- Output transform

与 Input transform 类似，也是对 16 个 6x6 的 tile 矩阵进行操作，最终与矩阵 A 相乘，得到 16 个 4x4 的矩阵，并保存到结果矩阵中。这里也大量使用 avx 指令进行循环展开，并在函数外层使用 openmp 多线程对不同区域进行操作。

这一步的优化包括算法优化、指令集优化，并且引入了 MKL 替代原来的矩阵乘法。在算法实现方面注意调整内存的分布，尽量提高缓存命中；尽量使用循环展开，用代码量和可读性换性能；通过分 tile、分块操作的方法，将算法分割成一个个独立的部分，在保证正确性的前提下尽量提高并行度。这些操作提升了 13 倍左右 (170GFlops->2200GFlops)

最后的性能，编译指令

## 参考资料
https://zhuanlan.zhihu.com/p/72953129

## 第二题：H5bench

### 如何测得正确的带宽
测量的精度是控制的上限，在I\O相关的问题中，如果不能准确地测量性能，就难以进行真正的改进，无法准确地分析瓶颈。因此我们对Linux I\O进行了一定的调研，分析了可能影响带宽的因素、如何测量带宽和性能抖动的原因。从这些调研出发，也能引出我们进行优化的主要方向。

● Page Cache
在Linux的实现中，文件Cache分为Page Cache和Buffer Cache，一个Page Cache包含若干个Buffer Cache。Page Cache主要用来作为文件系统上的文件数据的缓存来用，Buffer Cache则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。
当Linux内核开始一个读操作，比如，执行file->f_op->read()或file->f_op->write()，它会首先检查需要的数据是否在Page Cache中。如果命中cache，则可直接从内存中读取。因为访问磁盘的速度要远远低于访问内存的速度（ms和ns的差距），因此命中cache可以极大地提高I/O速度。
具体到Linux内核的读操作中，内核首先调用find_get_page()尝试在page cache中找到需要的数据。如果搜索的页并不在page cache中，find_get_page()返回NULL，并且内核将分配一个新页面，然后调用add_to_page_cache_lru()将之前搜索的页加入page cache中。最后调用readpage()将需要的数据从磁盘读入。
在对读取同一个文件的带宽进行多次测量时，需要运行
echo 3 >/proc/sys/vm/drop_caches
来手动清空Page Cache，否则内核存在直接从内存中读取数据的可能性，导致实测的I/O带宽失准。

● 异步I/O
异步I/O的工作方式是当进程读写文件时，一旦读写操作进入队列函数就结束，甚至有可能真正的I/O数据传输还没有开始，这样调用进程就可以在数据正在传输时继续自己的运行。
io_uring是2019年Linux5.1内核引入的取代传统Linux AIO的高性能异步I/O框架。通过全新的设计，共享内存，IO 过程不需要系统调用，由内核完成 IO 的提交， 以及 IO completion polling 机制，实现了高IOPS和高 Bandwidth。
h5bench提供了使用HDF5 Asynchronous I/O VOL connector为支撑的异步I/O模式。HDF5 Asynchronous I/O VOL connector利用HDF5异步接口，通过对I/O操作尽早调度以及与计算和通信并行执行，有效提高了HDF5的整体运行效率。

● 并行I/O
并行I/O是一种使用不同进程同时访问磁盘数据的技术，能够最大化带宽和速度。从Linux内核的角度看，块I/O操作的基本容器由bio结构体表示，其指向一个bio_vec结构体链表，这些结构体描述了每隔片段再物理页中的实际位置。块I/O层通过bi_idx域跟踪块I/O操作的完成进度，同时也起到分割bio结构体的作用，为I/O操作的并行执行提供条件。HDF5使用MPI-I/O的接口，从而支持了部分并行I/O操作，提供了比MPI-I/O更高级的数据抽象。

● 文件系统(RAID/NFS)
RAID通过将多块独立的磁盘（物理硬盘）按不同方式组合起来形成一个磁盘组（逻辑硬盘），从而提供比单个硬盘更高的I/O性能和数据冗余。不同的RAID级别有不同的I/O性能特点。如RAID5采用磁盘分段加奇偶校验技术，读出效率很高，写入效率一般。
若实际的文件系统为NFS，或是包含NFS的混合形态，那么实际的I/O带宽可能受网络带宽限制。通过网络传输的数据也需要的额外的校验操作，从而进一步限制I/O带宽。

● I/O测试/监控工具
FIO是一个可以产生很多线程或进程并执行用户指定的特定类型I/O操作的工具。FIO支持十几种不同类型的io引擎（libaio、sync、mmap、posixaio、network等等），可以测试块设备或文件，可以通过多线程或进程模拟各种io操作，可以测试统计iops、带宽和时延等性能。通过FIO相应模式测得的设备I/O带宽是我们可达到的带宽性能的极限。
系统设备IO负载情况的监控工具有iotop、iostat等。

● 性能抖动
SSD内部的I/O性能抖动：比特错误（随着使用时间增加，比特错误率越来越高，因比特错误导致的读延迟概率和时间也会随之增高）、读写/擦除操作冲突（当读写操作与擦除操作冲突会导致读写操作延迟）、垃圾回收（SSD内部进行GC时会占用磁盘资源）、读平衡（数据迁移与用户I/O之间的资源竞争）。
HDD内部的I/O性能抖动：机械硬盘性能受数据随机/顺序读写因素影响较大。此外，跨盘符时会出现性能抖动。
磁盘带宽占用：当其他进程当前任务竞争磁盘I/O时，会出现性能抖动。
内存占用：当系统中存在内存不足的压力时，Linux内核会利用flusher线程对Page Cache进行回写动作，势必会影响I/O性能，造成性能抖动。




### 优化方法
async 分块 压缩 多进程

## 总结
