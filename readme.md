## 配置环境

	a. 需要包含测试平台的硬件、软件环境信息，保证可复现性；

## 第一题报告
###　特性分析
- 缓存命中缓存一致性
### 优化思路
- 并行
卷积运算的过程是可以进行并行计算的，比如示例代码中给出的计算中，每一个channel的计算就是完全互不干扰，可以并行的。要想加速卷积运算，需要充分利用计算平台多核的优势，尽可能在每一个环节把计算并行化。我们在算法中通过openmp库，进行了并行计算的尝试。

- SIMD/向量化
单指令流多数据流（英语：Single Instruction Multiple Data，缩写：SIMD）是基于硬件的支持，微处理器中一个控制器可以控制多个平行的处理微元，对一组数据（数据向量）进行计算，是一种常用于加速计算密集的并行任务的优化方法。
1997年，x86扩展出了MMX指令集，伴随着80-bit的vector寄存器，首开向量化计算的先河。 之后，x86又扩展出了SSE和AVX指令集。SSE和AVX各有16个寄存器。SSE的16个寄存器为XMM0-XMM15，AVX的16个寄存器为YMM0-YMM15。同一个寄存器可以装载多个float数，并且可以用相应的指令同时进行运算。在我们的服务器和赛方提供的服务器上，我们依次实验了AVX128，AVX256和AVX512指令集。
算法优化
本题中采用的Winograd算法是一种减少乘法次数，增加加法次数，从而对矩阵乘法加速的算法。在示例代码中的winograd算法实际上是一种一维算法，需要逐个对每个图片每个channel的每一行每一列与每个卷积核相乘，因此有大量的for循环。通过查阅文献，我们发现在3D卷积运算中，相对于示例代码我们可以做出三个优化：
  - 转换1D的winograd算法到2D。比如示例的winograd(2,3)可以转换成winograd(2x2,, 3x3),可以同时计算得到一个(2x2)的输出。
  - 转换1D的winograd算法到3D。相关文献指出，在3D的矩阵运算中，可以把winograd算法分成4步，分别是Input transform、Filter transform、Matrix Multiplication、Output transform。
![](https://s2.ax1x.com/2019/04/29/E1qSoR.png)
  - 扩展winograd(2,3)算法到winograd(4,3)。根据相关文献指出，4x3算法能更多地减少乘法运算的比例，从而带来更好的加速效果。而且4x3将矩阵分块更大一些，对增加缓存命中也相对友好。

根据文献的实验结果，3D-winograd(2,3)算法是最节能的，适合在FPGA等边缘设备上部署。但3D-winograd(4,3)算法前向传播是最快的，因此我们最终采取了3D-winograd(4,3)算法。


- 缓存/cache miss
在计算密集的并行计算中，要尽可能地利用好缓存，精心设计程序以减少缓存到内存的数据读取。同时多线程的环境下还会出现伪共享问题，大大降低程序的性能。

# 实现/工程细节
- 缓存

在优化时，我们按照程序中的提示，先从优化矩阵计算出发。示例代码的矩阵运算显然是一种对缓存的低效利用，每次读取一段内存，但只用到了其中的第一个数据，下次循环又要去读取新的内存。通过交换循环的顺序，即可提升对缓存的利用效率。我们对矩阵乘法运算做了如下的改进，程序性能就有了10倍的提升(0.22GFlops->3.05GFlops)。

- simd avx128

把矩阵运算划分成一部分4x4矩阵运算，不能被4整除的部分单独做1x1的矩阵运算。并通过SIMD-avx128指令集计算4x4矩阵运算，在avx128中一个寄存器可以存储4个float数，也可以同时对4的float做向量化计算。在4x4的运算中没有循环，全部做循环展开做计算。同时加入fma指令集引入积和熔加运算，进一步压缩指令。此处优化有近6倍的提升(3.05GFlops->12.88 GFlops)

- 多线程 无效到有效 分配
  
此时，我们尝试通过引入openmp多线程进行矩阵运算，但多线程一直有负优化的效果，甚至经过实验发现有线程数加倍性能减半的现象。经过分析发现在示例代码中有大量的小矩阵运算，比如在第一部分逐channel逐卷积核的矩阵相乘中有大量的大小为4的矩阵，矩阵太小导致分给每个线程的任务过少，线程本身的开销大于多线程带来的收益，因此多线程在这种运算中是一种负优化。

为了引入并行加快速度，我们单独为中间的大矩阵运算编写了加入了多线程的函数，小矩阵运算则不加多线程。这一调整有近5倍的提升。(12.88GFlops->57.11GFlops)

- perf 调优，寻找瓶颈
此时，我们通过perf查看程序各部分的耗时分布。发现如下的问题：
	- 矩阵初始化耗时过长，应该改为memset，这一调整有3%的提升(57.11GFlops->60.86GFlops)
	- winograd函数中大矩阵计算前后的两个部分耗时过长，这一部分属于算法设计问题，应该对整个算法进行调整。
  
- 加入avx256
此时我们更换了一台支持avx2指令集的服务器同时指令集换成avx256，在avx256中每个寄存器可以存储8个float数，因此将原来4x4矩阵运算更换成8x8矩阵运算，这一调整有1.5倍的提升(60.86GFlops->97GFlops)。


编译器优化
llvm gcc尝试 (97-106GFlops)



升级到avx512 win4x3优化 && 3d矩阵乘法优化

最终在赛方提供的服务器上，我们重新写了winograd算法，更新成3D-winograd(4x3)算法。在这里，我们分四个部分详细介绍该算法的实现
- Input transform

首先把输入矩阵拆分成tiles，在这里tiles是长宽均为6的矩阵，在拆分过程中为了充分发挥向量化的作用，每一次会同时对16个6x6的tile矩阵进行操作，因为每个avx512寄存器能存储16个float数。通过循环展开手写avx指令的方式，把16个矩阵中的数读取出来并重新排列，之后再批量与矩阵G相乘，相乘操作也全部展开用avx指令的方式实现。这样批量操作可以充分发挥单核SIMD指令的效能，并且每个计算单元不会影响其他部分的内存，不会出现伪共享问题，可以实现完全的并行操作。大量的循环展开虽然增加了代码量，但有利于减少循环开销、也有利于指令流水线的调度。对于边长不满足被步长整除（步长为4/64，即无法被4/64整除）的参数，可以进行padding。
- Filter transform
因为卷积核是固定的3x3大小，SIMD无法发挥作用，因此这里使用了循环展开和多线程进行计算卷积核与矩阵G相乘，因为卷积核之间也是互相独立的，因此可以尽可能的增加并行度加快运算。最后的赋值也使用#pragma unroll指示编译器进行循环展开。

- Matrix Multiplication

经过测试发现，使用mkl替换我们自己的矩阵有较大的提升（约3倍提升），因此这里全部替换成mkl的sgemm函数。这里的并行度有两个参数，一个是36，因为经过Input transform得到的三维矩阵有一维是36，且彼此之间互不影响，可以独立并行。另一个参数是batch，因为之前把batch中所有图像拼在了一起，这里也可以并行，相当于对每个图像做计算。

- Output transform

与Input transform类似，也是对16个6x6的tile矩阵进行操作，最终与矩阵A相乘，得到16个4x4的矩阵，并保存到结果矩阵中。这里也大量使用avx指令进行循环展开，并在函数外层使用openmp多线程对不同区域进行操作。

这一步的优化包括算法优化、指令集优化，并且引入了MKL替代原来的矩阵乘法。在算法实现方面注意调整内存的分布，尽量提高缓存命中；尽量使用循环展开，用代码量和可读性换性能；通过分tile、分块操作的方法，将算法分割成一个个独立的部分，在保证正确性的前提下尽量提高并行度。这些操作提升了13倍左右(170GFlops->2200GFlops)


最后的性能，编译指令

## 参考资料
https://zhuanlan.zhihu.com/p/72953129

## 第二题报告
如何测得正确的带宽
async 分块 压缩 多进程

## 总结


